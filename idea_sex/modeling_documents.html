<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
</head>
<body>
<h1 id="topic-modeling-documents">Topic: Modeling Documents</h1>
<ol style="list-style-type: decimal">
<li><p>It seems like pretty good method is to use neural nets. My understanding is that there a few import aspects to this particular neural net. First, the input and output to the neural net are the same (the words of the document). Second, there needs a bottlenet somewhere in the neural net. We think of the bottleneck as our representation of the document. The first part of the neural net compresses the document into a single node. Then the second part of the neural net takes the single node and decompresses back into a document. One question I have is this: how important is it that we can reconstruct the document from the mathematical representation? Is our focus to &quot;<em>compress the document as much as possible while still being able to reconstruct the document with some degree of precision</em>&quot;? That doesn't necessarily seem like: that's a job for information theorists(?). Instead, perhaps what we should be aiming for is a semantic encapsulation of the document: that is, a vector represenation of the &quot;essence&quot; of the document, that isn't isomorphic in anyway to the document. Instead of giving something from which we can reconstruct the document, it gives us something that we can use to operate with. We can add and subtract documents, and preserver something (maybe meaning/semantic value. We can compare documents, an analog of cosine similarity in word2vec. We can compute various metrics of the document - like breadth or depth of the document's content (I can imagine a bunch of other cool metrics).</p>
<p>At the same time, I can imagine in simply &quot;compressing&quot; the document, these features are preserved, but I don't think that these features will be preserved naturally. I think the compression method probably needs to be specficially engineered to preserve these features.</p></li>
<li><p>The final sentiment of point 1 seems to suggest that we should look into the construction of word2vec, and think of analogs to the document side.</p></li>
<li><p>Our null hypothesis should be some comparison of a model that uses bag of words. If we were treating this as a philosophy-math project, our claim that we are trying to prove might be: &quot;considering the ordering words gives a better model of the document than bag of words&quot;.</p></li>
<li><p>I think I'm getting confused. Are we trying to create a model that is better than bag of words? Or we are we trying to create a model on which we can apply operations? I'm assumign the latter - because the forming is less interesting and answered if we solve the latter.</p></li>
<li><p>I'm intrigued by the idea of treating a <em>ocument as an evolving system</em>. We think of the first word or sentence as the genesis unit. And then traits are carried throughout the document. In particular, I think it may be some analog of an evolving system because there is (a) traits that are passed down through sentence, (2) a notion of time - a sentence in the past can't influence a sentence in the future -, (3) I don't know the other defining features of evolution, but I bet that documents definitely don't have all of them. Maybe we could describe it as a quasi-evolutionary systmem. This idea may have cool insights into modeling it.</p></li>
<li><p>I would imagine there is not very much work done on &quot;compressing evolutionary systems&quot;, but that is a cool topic.<br /> I suppose that they are complex systems exhibiting weak emergence, so the entire idea of compressing them is theoretically impossible.</p></li>
<li><p>I haven't mentioned the composition problem. I'm starting to think that thinking of document analysis in terms of composition problem is the wrong way to go. That's definitely what the tree-based neural net does, but I think it's arguable if the RNN is actually solving hte composition problem (like it defines a meaning for &quot;the cat&quot;).</p></li>
<li><p>One way to encode a document would be to construct a markov chain, (view it as a matrix) and then perform PCA and see what we get out. That wouldn't allow us to compare document per se. Or actually, I can think of a few ways that it could. But again, this neglects word ordering (to some extent) by only considering 1 word previous (or however you define the markov process). What we are looking for is something that can be a markov chain but also consider a dynamic (or arbitrary) number of previous words. Example: &quot;dna polymerase&quot; doesn't need previous words for a lot its meaning, but &quot;the&quot; does, and &quot;the&quot; in different situations perhaps needs various numbers of previous words. I think this might be something that you could build into the markov model. Idea: build the markov model, and then add in the number of words to look back for each word by taking random walks on the markov chain and comparing them to sentences in the document. Alternatively, build the markov model, and then look at the distribution of inedges and outedges to a given word. Based on that distribution, come up with a model for how far back a certain word needs to look in various situation.s I'm imagining a nondeterministic, iterative algorithm like pagerank for the extra sutff that we are throwing on top of the markov chain.</p></li>
</ol>
</body>
</html>
